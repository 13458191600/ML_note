{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c19abd",
   "metadata": {},
   "source": [
    "# 6、支持向量机\n",
    "\n",
    "支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。\n",
    "\n",
    "## 6.1 函数间隔与几何间隔\n",
    "\n",
    "对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：\n",
    "\n",
    "![1.png](https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png)\n",
    "\n",
    "对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。\n",
    "\n",
    "![2.png](https://i.loli.net/2018/10/17/5bc72f6a06d5a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195675b",
   "metadata": {},
   "source": [
    "### 6.1.1 函数间隔 \n",
    "![3.png](imgs/1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd175b6b",
   "metadata": {},
   "source": [
    "### 6.1.2 几何间隔\n",
    "\n",
    "**几何间隔**代表的则是数据点到超平面的真实距离，对于超平面w'x+b=0，w代表的是该超平面的法向量，设x*为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x*=x-r(w/||w||)，又x*在超平面上，即w'x*+b=0，代入即可得：\n",
    "\n",
    "![5.png](https://i.loli.net/2018/10/17/5bc72f697d499.png)\n",
    "\n",
    "为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：\n",
    "\n",
    "![6.png](https://i.loli.net/2018/10/17/5bc72f696fd10.png)\n",
    "\n",
    "从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w'x+b|，而几何间隔就是点到超平面的距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e75914",
   "metadata": {},
   "source": [
    "## 6.2 最大间隔与支持向量\n",
    "\n",
    "通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：\n",
    "\n",
    "![7.png](https://i.loli.net/2018/10/17/5bc72f69af163.png)\n",
    "\n",
    "一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：\n",
    "\n",
    "![8.png](https://i.loli.net/2018/10/17/5bc72f697bb1d.png)\n",
    "\n",
    "对于y(w'x+b)=1的数据点，即下图中位于w'x+b=1或w'x+b=-1上的数据点，我们称之为**支持向量**（support vector），易知：对于所有的支持向量，它们恰好满足y*(w'x*+b)=1，而所有不是支持向量的点，有y*(w'x*+b)>1。两个异类支持向量到超平面的距离叫间隔，即$d=\\frac{2}{||w||}$\n",
    "\n",
    "![9.png](https://i.loli.net/2018/10/17/5bc72f6a838c4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2d0b9",
   "metadata": {},
   "source": [
    "## 6.3 从原始优化问题到对偶问题\n",
    "\n",
    "对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：\n",
    "\n",
    "![10.png](https://i.loli.net/2018/10/17/5bc72f6978cbb.png)\n",
    "\n",
    "即变为了一个带约束的凸二次规划问题，\n",
    "![11.png](imgs/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b15bd3",
   "metadata": {},
   "source": [
    "## 6.9\n",
    "$$\\boldsymbol{w} = \\sum_{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i$$\n",
    "[推导]：公式(6.8)可作如下展开\n",
    "$$\\begin{aligned}\n",
    "L(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) &= \\frac{1}{2}||\\boldsymbol{w}||^2+\\sum_{i=1}^m\\alpha_i(1-y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)) \\\\\n",
    "& =  \\frac{1}{2}||\\boldsymbol{w}||^2+\\sum_{i=1}^m(\\alpha_i-\\alpha_iy_i \\boldsymbol{w}^T\\boldsymbol{x}_i-\\alpha_iy_ib)\\\\\n",
    "& =\\frac{1}{2}\\boldsymbol{w}^T\\boldsymbol{w}+\\sum_{i=1}^m\\alpha_i -\\sum_{i=1}^m\\alpha_iy_i\\boldsymbol{w}^T\\boldsymbol{x}_i-\\sum_{i=1}^m\\alpha_iy_ib\n",
    "\\end{aligned}​$$\n",
    "对$\\boldsymbol{w}$和$b$分别求偏导数​并令其等于0\n",
    "$$\\frac {\\partial L}{\\partial \\boldsymbol{w}}=\\frac{1}{2}\\times2\\times\\boldsymbol{w} + 0 - \\sum_{i=1}^{m}\\alpha_iy_i \\boldsymbol{x}_i-0= 0 \\Longrightarrow \\boldsymbol{w}=\\sum_{i=1}^{m}\\alpha_iy_i \\boldsymbol{x}_i$$\n",
    "\n",
    "$$\\frac {\\partial L}{\\partial b}=0+0-0-\\sum_{i=1}^{m}\\alpha_iy_i=0  \\Longrightarrow  \\sum_{i=1}^{m}\\alpha_iy_i=0$$\n",
    "值得一提的是，上述求解过程遵循的是西瓜书附录B中公式(B.7)左边的那段话“在推导对偶问题时，常通过将拉格朗日函数$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$对$\\boldsymbol{x}$求导并令导数为0，来获得对偶函数的表达形式”。那么这段话背后的缘由是啥呢？在这里我认为有两种说法可以进行解释：\n",
    "1. 对于强对偶性成立的优化问题，其主问题的最优解$\\boldsymbol{x}^*$一定满足附录①给出的KKT条件（证明参见参考文献[3]的§ 5.5），而KKT条件中的条件(1)就要求最优解$\\boldsymbol{x}^*$能使得拉格朗日函数$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$关于$\\boldsymbol{x}$的一阶导数等于0；\n",
    "2. 对于任意优化问题，若拉格朗日函数$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$是关于$\\boldsymbol{x}$的凸函数，那么此时对$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$关于$\\boldsymbol{x}$求导并令导数等于0解出来的点一定是最小值点。根据对偶函数的定义可知，将最小值点代回$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$即可得到对偶函数。\n",
    "\n",
    "显然，对于SVM来说，从以上任意一种说法都能解释得通。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15282a6",
   "metadata": {},
   "source": [
    "对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：\n",
    "\n",
    "![11.png](https://i.loli.net/2018/10/17/5bc72f9332be7.png)\n",
    "\n",
    "上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：\n",
    "\n",
    "![12.png](https://i.loli.net/2018/10/17/5bc72f93321c5.png)\n",
    "\n",
    "由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：\n",
    "\n",
    "![13.png](https://i.loli.net/2018/10/17/5bc72f9330967.png)\n",
    "\n",
    "这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。\n",
    "\n",
    "（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：\n",
    "\n",
    "![14.png](https://i.loli.net/2018/10/17/5bc72f9333e66.png)\n",
    "\n",
    "将上述结果代入L得到：\n",
    "\n",
    "![15.png](https://i.loli.net/2018/10/17/5bc72f935ae21.png)\n",
    "\n",
    "（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。\n",
    "\n",
    "![16.png](https://i.loli.net/2018/10/17/5bc72f9338a9d.png)\n",
    "\n",
    "（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。\n",
    "\n",
    "![17.png](https://i.loli.net/2018/10/17/5bc72f93419ca.png)\n",
    "\n",
    "在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w'x+b中，若f(x)>0，则为正类，f(x)<0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。\n",
    "\n",
    "![18.png](https://i.loli.net/2018/10/17/5bc72f9353166.png)\n",
    "\n",
    "这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w'x+b)-1≥0，满足：\n",
    "\n",
    "![19.png](https://i.loli.net/2018/10/17/5bc72f933c947.png)        \n",
    "\n",
    "##**6.4 核函数**\n",
    "\n",
    "由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用**映射**的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：\n",
    "\n",
    "![20.png](https://i.loli.net/2018/10/17/5bc72f934303e.png)\n",
    "\n",
    "按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：\n",
    "\n",
    "（1）原对偶问题变为：\n",
    "\n",
    "![21.png](https://i.loli.net/2018/10/17/5bc730cc68b3b.png)\n",
    "\n",
    "（2）原分类函数变为：\n",
    "​    ![22.png](https://i.loli.net/2018/10/17/5bc730cc1b673.png)\n",
    "\n",
    "求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了**核函数**（Kernel）的概念。\n",
    "\n",
    "![23.png](https://i.loli.net/2018/10/17/5bc730cc49adc.png)\n",
    "\n",
    "因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效**（低维计算，高维表现）**，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：\n",
    "\n",
    "（1）对偶问题：\n",
    "\n",
    "![24.png](https://i.loli.net/2018/10/17/5bc730cc173b2.png)\n",
    "\n",
    "（2）分类函数：\n",
    "\n",
    "![25.png](https://i.loli.net/2018/10/17/5bc730cc05959.png)\n",
    "\n",
    "因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：\n",
    "\n",
    "![26.png](https://i.loli.net/2018/10/17/5bc730ccc468c.png)\n",
    "\n",
    "由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：\n",
    "\n",
    "![27.png](https://i.loli.net/2018/10/17/5bc730ccc541a.png)\n",
    "\n",
    "##**6.5 软间隔支持向量机**\n",
    "\n",
    "前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有**噪声**的情形，噪声数据（**outlier**）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。\n",
    "\n",
    "![28.png](https://i.loli.net/2018/10/17/5bc730ccce68e.png)\n",
    "\n",
    "为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了**“软间隔”支持向量机**的概念\n",
    "\n",
    "\t* 允许某些数据点不满足约束y(w'x+b)≥1；\n",
    "\t* 同时又使得不满足约束的样本尽可能少。\n",
    "\n",
    "这样优化目标变为：\n",
    "\n",
    "![29.png](https://i.loli.net/2018/10/17/5bc730cc6c9fe.png)\n",
    "\n",
    "如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。\n",
    "\n",
    "![30.png](https://i.loli.net/2018/10/17/5bc730cc5e5a9.png)\n",
    "\n",
    "支持向量机中的损失函数为**hinge损失**，引入**“松弛变量”**，目标函数与约束条件可以写为：\n",
    "\n",
    "![31.png](https://i.loli.net/2018/10/17/5bc7317aa3411.png)\n",
    "\n",
    "其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：\n",
    "\n",
    "![32.png](https://i.loli.net/2018/10/17/5bc7317a4c96e.png)\n",
    "\n",
    "按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：\n",
    "\n",
    "![33.png](https://i.loli.net/2018/10/17/5bc7317a6dff2.png)\n",
    "\n",
    "将w代入L化简，便得到其对偶问题：\n",
    "\n",
    "![34.png](https://i.loli.net/2018/10/17/5bc7317ab6646.png)\n",
    "\n",
    "将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。\n",
    "\n",
    "----在此SVM就介绍完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4f302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
