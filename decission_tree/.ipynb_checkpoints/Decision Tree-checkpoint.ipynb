{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdd09de",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#决策树\" data-toc-modified-id=\"决策树-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>决策树</a></span><ul class=\"toc-item\"><li><span><a href=\"#概览\" data-toc-modified-id=\"概览-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>概览</a></span></li><li><span><a href=\"#算法流程\" data-toc-modified-id=\"算法流程-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>算法流程</a></span><ul class=\"toc-item\"><li><span><a href=\"#原理\" data-toc-modified-id=\"原理-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>原理</a></span></li><li><span><a href=\"#流程\" data-toc-modified-id=\"流程-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>流程</a></span></li></ul></li><li><span><a href=\"#划分选择\" data-toc-modified-id=\"划分选择-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>划分选择</a></span><ul class=\"toc-item\"><li><span><a href=\"#信息增益\" data-toc-modified-id=\"信息增益-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>信息增益</a></span><ul class=\"toc-item\"><li><span><a href=\"#自信息\" data-toc-modified-id=\"自信息-1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>自信息</a></span></li><li><span><a href=\"#信息熵（ID3决策树算法）\" data-toc-modified-id=\"信息熵（ID3决策树算法）-1.3.1.2\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>信息熵（ID3决策树算法）</a></span></li><li><span><a href=\"#信息增益\" data-toc-modified-id=\"信息增益-1.3.1.3\"><span class=\"toc-item-num\">1.3.1.3&nbsp;&nbsp;</span>信息增益</a></span></li><li><span><a href=\"#缺点\" data-toc-modified-id=\"缺点-1.3.1.4\"><span class=\"toc-item-num\">1.3.1.4&nbsp;&nbsp;</span>缺点</a></span></li></ul></li><li><span><a href=\"#Information-gain-calculation-信息增益率\" data-toc-modified-id=\"Information-gain-calculation-信息增益率-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Information gain calculation 信息增益率</a></span></li><li><span><a href=\"#基尼系数\" data-toc-modified-id=\"基尼系数-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>基尼系数</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183fea46",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523b011",
   "metadata": {},
   "source": [
    "## 概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa8626",
   "metadata": {},
   "source": [
    "决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965a7c5",
   "metadata": {},
   "source": [
    "## 算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5c5b1",
   "metadata": {},
   "source": [
    "### 原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37772901",
   "metadata": {},
   "source": [
    "输入：训练集 $D=\\{ (X_1,y_1),(X_2,y_2),...(X_m,y_m),\\}$\n",
    "     属性集 $A=\\{a_1,a_2,...,a_n\\}$\n",
    "   \n",
    "def createBranch(D,A):\n",
    "\n",
    "'''\n",
    "\n",
    "此处运用了迭代的思想。\n",
    "\n",
    "'''\n",
    "    \n",
    "    生成节点 node\n",
    "    \n",
    "    if 数据集D中的所有数据的分类标签相同都是C:\n",
    "        将node标记为c类叶节点\n",
    "        return  node\n",
    "    if A为空 or D在A上的属性取值相同：\n",
    "        将node标记为D中最多的标签节点\n",
    "        return  node\n",
    "    else:\n",
    "        寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）$a_*$\n",
    "        划分数据集 $D_v$为在$a_*$上取值为$a_*^v$的样本集\n",
    "        创建分支节点\n",
    "            for 每个划分的子集$D_v$\n",
    "                分支节点 = createBranch($D_v,A\\a_*$) \n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b5f08",
   "metadata": {},
   "source": [
    "###  流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d64af",
   "metadata": {},
   "source": [
    "收集数据: 可以使用任何方法。\\\n",
    "准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)\\\n",
    "分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。\\\n",
    "训练算法: 构造树的数据结构。\\\n",
    "测试算法: 使用训练好的树计算错误率。\\\n",
    "使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed01d1",
   "metadata": {},
   "source": [
    "## 划分选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1291edb",
   "metadata": {},
   "source": [
    "目标：使得样本越来越越纯，分支节点包含的样本尽可能属于同一类别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc660a86",
   "metadata": {},
   "source": [
    "### 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e567a87",
   "metadata": {},
   "source": [
    "#### 自信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee4515",
   "metadata": {},
   "source": [
    "$$I(X)= -log_bP(x)$$\n",
    "b=2 单位为bit(二进制储存） b=e（理论上存储效率最高）单位为 nat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3589e2",
   "metadata": {},
   "source": [
    "#### 信息熵（ID3决策树算法）\n",
    "自信息的数学期望，信息熵越大，X的不确定性越大\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c84d87",
   "metadata": {},
   "source": [
    "$$\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|}p_k\\log_{2}{p_k}$$\n",
    "约定P（X）=0 时$-\\sum_{k=1}^{|\\mathcal{Y}|}p_k\\log_{2}{p_k}=0$(取极限)\\\n",
    "当X为均匀分布时，信息熵最大，证明如下\\\n",
    "证明：$0\\leq\\operatorname{Ent}(D)\\leq\\log_{2}|\\mathcal{Y}|$：\n",
    "已知集合$D$的信息熵的定义为\n",
    "$$\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}$$\n",
    "其中，$|\\mathcal{Y}|$表示样本类别总数，$p_k$表示第$k$类样本所占的比例，且$0 \\leq p_k \\leq 1,\\sum_{k=1}^{n}p_k=1$。若令$|\\mathcal{Y}|=n,p_k=x_k$，那么信息熵$\\operatorname{Ent}(D)$就可以看作一个$n$元实值函数，也即\n",
    "$$\\operatorname{Ent}(D)=f(x_1,...,x_n)=-\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k} $$\n",
    "其中，$0 \\leq x_k \\leq 1,\\sum_{k=1}^{n}x_k=1$，下面考虑求该多元函数的最值。首先我们先来求最大值，如果不考虑约束$0 \\leq x_k \\leq 1$，仅考虑$\\sum_{k=1}^{n}x_k=1$的话，对$f(x_1,...,x_n)$求最大值等价于如下最小化问题\n",
    "$$\\begin{array}{ll}{\n",
    "\\operatorname{min}} & {\\sum\\limits_{k=1}^{n} x_{k} \\log _{2} x_{k} } \\\\ \n",
    "{\\text { s.t. }} & {\\sum\\limits_{k=1}^{n}x_k=1} \n",
    "\\end{array}$$\n",
    "显然，在$0 \\leq x_k \\leq 1$时，此问题为凸优化问题，而对于凸优化问题来说，能令其拉格朗日函数的一阶偏导数等于0的点即为最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为\n",
    "$$L(x_1,...,x_n,\\lambda)=\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)$$\n",
    "其中，$\\lambda$为拉格朗日乘子。对$L(x_1,...,x_n,\\lambda)$分别关于$x_1,...,x_n,\\lambda$求一阶偏导数，并令偏导数等于0可得\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial x_1}&=\\cfrac{\\partial }{\\partial x_1}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n",
    "&=\\log _{2} x_{1}+x_1\\cdot \\cfrac{1}{x_1\\ln2}+\\lambda=0 \\\\\n",
    "&=\\log _{2} x_{1}+\\cfrac{1}{\\ln2}+\\lambda=0 \\\\\n",
    "&\\Rightarrow \\lambda=-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}\\\\\n",
    "\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial x_2}&=\\cfrac{\\partial }{\\partial x_2}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n",
    "&\\Rightarrow \\lambda=-\\log _{2} x_{2}-\\cfrac{1}{\\ln2}\\\\\n",
    "\\vdots\\\\\n",
    "\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial x_n}&=\\cfrac{\\partial }{\\partial x_n}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n",
    "&\\Rightarrow \\lambda=-\\log _{2} x_{n}-\\cfrac{1}{\\ln2}\\\\\n",
    "\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial \\lambda}&=\\cfrac{\\partial }{\\partial \\lambda}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n",
    "&\\Rightarrow \\sum_{k=1}^{n}x_k=1\\\\\n",
    "\\end{aligned}$$\n",
    "整理一下可得\n",
    "$$\\left\\{ \\begin{array}{lr}\n",
    "\\lambda=-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}=-\\log _{2} x_{2}-\\cfrac{1}{\\ln2}=...=-\\log _{2} x_{n}-\\cfrac{1}{\\ln2} \\\\\n",
    "\\sum\\limits_{k=1}^{n}x_k=1\n",
    "\\end{array}\\right.$$\n",
    "由以上两个方程可以解得\n",
    "$$x_1=x_2=...=x_n=\\cfrac{1}{n}$$\n",
    "又因为$x_k$还需满足约束$0 \\leq x_k \\leq 1$，显然$0 \\leq\\cfrac{1}{n}\\leq 1$，所以$x_1=x_2=...=x_n=\\cfrac{1}{n}$是满足所有约束的最优解，也即为当前最小化问题的最小值点，同时也是$f(x_1,...,x_n)$的最大值点。将$x_1=x_2=...=x_n=\\cfrac{1}{n}$代入$f(x_1,...,x_n)$中可得\n",
    "$$f(\\cfrac{1}{n},...,\\cfrac{1}{n})=-\\sum_{k=1}^{n} \\cfrac{1}{n} \\log _{2} \\cfrac{1}{n}=-n\\cdot\\cfrac{1}{n} \\log _{2} \\cfrac{1}{n}=\\log _{2} n$$\n",
    "所以$f(x_1,...,x_n)$在满足约束$0 \\leq x_k \\leq 1,\\sum_{k=1}^{n}x_k=1$时的最大值为$\\log _{2} n$。求完最大值后下面我们再来求最小值，如果不考虑约束$\\sum_{k=1}^{n}x_k=1$，仅考虑$0 \\leq x_k \\leq 1$的话，$f(x_1,...,x_n)$可以看做是$n$个互不相关的一元函数的加和，也即\n",
    "$$f(x_1,...,x_n)=\\sum_{k=1}^{n} g(x_k) $$\n",
    "其中，$g(x_k)=-x_{k} \\log _{2} x_{k},0 \\leq x_k \\leq 1$。那么当$g(x_1),g(x_2),...,g(x_n)$分别取到其最小值时，$f(x_1,...,x_n)$也就取到了最小值。所以接下来考虑分别求$g(x_1),g(x_2),...,g(x_n)$各自的最小值，由于$g(x_1),g(x_2),...,g(x_n)$的定义域和函数表达式均相同，所以只需求出$g(x_1)$的最小值也就求出了$g(x_2),...,g(x_n)$的最小值。下面考虑求$g(x_1)$的最小值，首先对$g(x_1)$关于$x_1$求一阶和二阶导数\n",
    "$$g^{\\prime}(x_1)=\\cfrac{d(-x_{1} \\log _{2} x_{1})}{d x_1}=-\\log _{2} x_{1}-x_1\\cdot \\cfrac{1}{x_1\\ln2}=-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}$$\n",
    "$$g^{\\prime\\prime}(x_1)=\\cfrac{d\\left(g^{\\prime}(x_1)\\right)}{d x_1}=\\cfrac{d\\left(-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}\\right)}{d x_1}=-\\cfrac{1}{x_{1}\\ln2}$$\n",
    "显然，当$0 \\leq x_k \\leq 1$时$g^{\\prime\\prime}(x_1)=-\\cfrac{1}{x_{1}\\ln2}$恒小于0，所以$g(x_1)$是一个在其定义域范围内开口向下的凹函数，那么其最小值必然在边界取，于是分别取$x_1=0$和$x_1=1$，代入$g(x_1)$可得\n",
    "$$g(0)=-0\\log _{2} 0=0$$\n",
    "$$g(1)=-1\\log _{2} 1=0$$\n",
    "所以，$g(x_1)$的最小值为0，同理可得$g(x_2),...,g(x_n)$的最小值也为0，那么$f(x_1,...,x_n)$的最小值此时也为0。但是，此时是不考虑约束$\\sum_{k=1}^{n}x_k=1$，仅考虑$0 \\leq x_k \\leq 1$时取到的最小值，若考虑约束$\\sum_{k=1}^{n}x_k=1$的话，那么$f(x_1,...,x_n)$的最小值一定大于等于0。如果令某个$x_k=1$，那么根据约束$\\sum_{k=1}^{n}x_k=1$可知$x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$，将其代入$f(x_1,...,x_n)$可得\n",
    "$$f(0,0,...,0,1,0,...,0)=-0 \\log _{2}0-0 \\log _{2}0...-0 \\log _{2}0-1 \\log _{2}1-0 \\log _{2}0...-0 \\log _{2}0=0 $$\n",
    "所以$x_k=1,x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$一定是$f(x_1,...,x_n)$在满足约束$\\sum_{k=1}^{n}x_k=1$和$0 \\leq x_k \\leq 1$的条件下的最小值点，其最小值为0。<br>\n",
    "综上可知，当$f(x_1,...,x_n)$取到最大值时：$x_1=x_2=...=x_n=\\cfrac{1}{n}$，此时样本集合纯度最低；当$f(x_1,...,x_n)$取到最小值时：$x_k=1,x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$，此时样本集合纯度最高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e4f9",
   "metadata": {},
   "source": [
    "#### 信息增益\n",
    "寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）$a_*$\\\n",
    "划分数据集 $D_v$为在$a_*$上取值为$a_*^v$的样本集\\\n",
    "$$\\operatorname{Gain}(D,a) = \\operatorname{Ent}(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}\\operatorname{Ent}({D^v})$$\n",
    "$${\\displaystyle {\\displaystyle I(X;Y)=D_{\\mathrm {KL} }(p(x,y)\\|p(x)\\otimes p(y))}}$$\n",
    "\n",
    "这个是信息增益的定义公式，在信息论中信息增益也称为\n",
    "##### <a href=\"https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF\">互信息</a>，\n",
    "\n",
    "设随机变量${\\displaystyle {\\displaystyle (X,Y)}}{\\displaystyle {\\displaystyle (X,Y)}}$是空间${\\displaystyle {\\displaystyle {\\mathcal {X}}\\times {\\mathcal {Y}}}}{\\displaystyle {\\displaystyle {\\mathcal {X}}\\times {\\mathcal {Y}}}}$中的一对随机变量。若他们的联合分布是${\\displaystyle {\\displaystyle p(x,y)}}{\\displaystyle {\\displaystyle p(x,y)}}$，边缘分布分别是${\\displaystyle {\\displaystyle p(x)}}{\\displaystyle {\\displaystyle p(x)}}$和${\\displaystyle {\\displaystyle p(y)}}{\\displaystyle {\\displaystyle p(y)}}$，那么，它们之间的互信息可以定义为：\n",
    "\n",
    "$${\\displaystyle {\\displaystyle I(X;Y)=D_{\\mathrm {KL} }(p(x,y)\\|p(x)\\otimes p(y))}}{\\displaystyle {\\displaystyle I(X;Y)=D_{\\mathrm {KL} }(p(x,y)\\|p(x)\\otimes p(y))}}$$\n",
    "其中，$${\\displaystyle {\\displaystyle D_{\\mathrm {KL} }}}$$为KL散度(Kullback–Leibler divergence)。注意，根据KL散度的性质，若联合分布${\\displaystyle p(x,y)}{\\displaystyle p(x,y)}$$等于边缘分布$${\\displaystyle p(x)}p(x)$$和$${\\displaystyle p(y)}p(y)$$的乘积，则$${\\displaystyle I(X;Y)=0}I(X;Y)=0$$，即当$${\\displaystyle X}X$$和$${\\displaystyle Y}Y$$相互独立的时候，观测到Y对于我们预测X没有任何帮助，此时他们的互信息为0。表示已知一个随机变量的信息后使得另一个随机变量的不确定性减少的程度。所以在这里，这个公式可以理解为在属性$a$的取值已知后，样本类别这个随机变量的不确定性减小的程度。若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，也即为“纯度提升”越大。\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2caa6c",
   "metadata": {},
   "source": [
    "##### 离散变量的互信息\n",
    "离散随机变量 X 和 Y 的互信息可以计算为：\n",
    "\n",
    "$$I(X;Y)=\\sum _{y\\in Y}\\sum _{x\\in X}p(x,y)\\log {({\\frac  {p(x,y)}{p(x)\\,p(y)}})}$$\n",
    "其中 $p(x, y)$ 是 X 和 Y 的联合概率质量函数，而${\\displaystyle p(x)}p(x) $和 ${\\displaystyle p(y)}p(y) $分别是 X 和 Y 的边缘概率质量函数。\n",
    "\n",
    "连续变量的互信息\n",
    "在连续随机变量的情形下，求和被替换成了二重定积分：\n",
    "\n",
    "$${\\displaystyle I(X;Y)=\\int _{Y}\\int _{X}p(x,y)\\log {\\left({\\frac {p(x,y)}{p(x)\\,p(y)}}\\right)}\\;dx\\,dy,}I(X;Y)=\\int _{Y}\\int _{X}p(x,y)\\log {\\left({\\frac  {p(x,y)}{p(x)\\,p(y)}}\\right)}\\;dx\\,dy,$$\n",
    "其中 p(x, y) 当前是 X 和 Y 的联合概率密度函数，而 {\\displaystyle p(x)}p(x) 和 {\\displaystyle p(y)}p(y) 分别是 X 和 Y 的边缘概率密度函数。\n",
    "\n",
    "如果对数以 2 为基底，互信息的单位是bit。\n",
    "\n",
    "直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。（这种情形的一个非常特殊的情况是当 X 和 Y 为相同随机变量时。）\n",
    "\n",
    "互信息是 X 和 Y 的联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：I(X; Y) = 0 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，p(x,y) = p(x) p(y)，因此：\n",
    "\n",
    "$${\\displaystyle \\log {\\left({\\frac {p(x,y)}{p(x)\\,p(y)}}\\right)}=\\log 1=0.\\,\\!}\\log {\\left({\\frac  {p(x,y)}{p(x)\\,p(y)}}\\right)}=\\log 1=0.\\,\\!$$\n",
    "此外，互信息是非负的（即 $${\\displaystyle I(X;Y)\\geq 0}{\\displaystyle I(X;Y)\\geq 0};$$ 见下文），而且是对称的（即 $${\\displaystyle I(X;Y)=I(Y;X)}{\\displaystyle I(X;Y)=I(Y;X)}）$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18047476",
   "metadata": {},
   "source": [
    "#### 缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61541d6",
   "metadata": {},
   "source": [
    "对可取值数目较多的属性有偏好，因为这样信息增益最大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ced243",
   "metadata": {},
   "source": [
    "### Information gain calculation 信息增益率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c1e64",
   "metadata": {},
   "source": [
    "$${\\displaystyle gain_ratio(D,a)=\\frac{Gain}{IV}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c28ed",
   "metadata": {},
   "source": [
    "其中IV=\n",
    "$${\\displaystyle IV(D,a)=-\\sum _{v\\in values(a)}{\\frac {|\\{x\\in D|value(x,a)=v\\}|}{|D|}}\\cdot \\log _{2}\\left({\\frac {|\\{x\\in D|value(x,a)=v\\}|}{|D|}}\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cbda4",
   "metadata": {},
   "source": [
    "信息增益率对可取值数目较少的属性有偏好（IV最小）\\ \n",
    "C4.5采用先选择 信息增益高于平均水平的属性，再选择信息增益率最高的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292673ca",
   "metadata": {},
   "source": [
    "### 基尼系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb384bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
