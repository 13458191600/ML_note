{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、线性模型**\n",
    "$$\n",
    "\n",
    "##**3.1 线性回归**\n",
    "\n",
    "线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000-->13亿...2016-->15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）-->15k。\n",
    "\n",
    "有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：\n",
    "\n",
    "- 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。\n",
    "\n",
    "- 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。\n",
    "\n",
    "（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：\n",
    "\n",
    "![2.png](https://i.loli.net/2018/10/17/5bc722b0ccec4.png)\n",
    "\n",
    "（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2...xd）,y}，则y=wx+b需要写成：\n",
    "\n",
    "![0.png](https://i.loli.net/2018/10/17/5bc72567b8bcd.png)\n",
    "\n",
    "通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：\n",
    "\n",
    "![3.png](https://i.loli.net/2018/10/17/5bc722b0ad8f7.png)\n",
    "\n",
    "![4.png](https://i.loli.net/2018/10/17/5bc722b0af652.png)\n",
    "\n",
    "![5.png](https://i.loli.net/2018/10/17/5bc722b090543.png)\n",
    "\n",
    "同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。\n",
    "\n",
    "![6.png](https://i.loli.net/2018/10/17/5bc722b0cde33.png)\n",
    "\n",
    "另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：\n",
    "\n",
    "![7.png](https://i.loli.net/2018/10/17/5bc722b103cbf.png)\n",
    "\n",
    "更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。\n",
    "\n",
    "![8.png](https://i.loli.net/2018/10/17/5bc722b0a2841.png)\n",
    "\n",
    "##**3.2 线性几率回归**\n",
    "\n",
    "回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。\n",
    "\n",
    "![9.png](https://i.loli.net/2018/10/17/5bc722b0c7748.png)\n",
    "\n",
    "![10.png](https://i.loli.net/2018/10/17/5bc722b0a655d.png)\n",
    "\n",
    "若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。\n",
    "\n",
    "![11.png](https://i.loli.net/2018/10/17/5bc723b824f0c.png)\n",
    "\n",
    "![12.png](https://i.loli.net/2018/10/17/5bc723b817961.png)\n",
    "\n",
    "\n",
    "\n",
    "##**3.3 线性判别分析**\n",
    "\n",
    "线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：\n",
    "\n",
    "![13.png](https://i.loli.net/2018/10/17/5bc723b863ebb.png)![14.png](https://i.loli.net/2018/10/17/5bc723b85bfa9.png)\n",
    "\n",
    "想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。\n",
    "\n",
    "+ 类内散度矩阵（within-class scatter matrix）\n",
    "\n",
    "![15.png](https://i.loli.net/2018/10/17/5bc723b8156e1.png)\n",
    "\n",
    "+ 类间散度矩阵(between-class scaltter matrix)\n",
    "\n",
    "![16.png](https://i.loli.net/2018/10/17/5bc723b7e9db3.png)\n",
    "\n",
    "因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。\n",
    "\n",
    "![17.png](https://i.loli.net/2018/10/17/5bc723b7e8a61.png)\n",
    "\n",
    "从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。\n",
    "\n",
    "![18.png](https://i.loli.net/2018/10/17/5bc723b83d5e0.png)\n",
    "\n",
    "若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    \n",
    "​             \n",
    "##**3.4 多分类学习**\n",
    "\n",
    "现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。\n",
    "\n",
    "+ OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。\n",
    "\n",
    "+ OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。\n",
    "\n",
    "+ MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。\n",
    "\n",
    "![19.png](https://i.loli.net/2018/10/17/5bc723b862bfb.png)\n",
    "\n",
    "![20.png](https://i.loli.net/2018/10/17/5bc723b8300d5.png)\n",
    "\n",
    "##**3.5 类别不平衡问题**\n",
    "\n",
    "类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：\n",
    "\n",
    "1.  在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。\n",
    "2.  在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。\n",
    "3.  直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。![21.png](https://i.loli.net/2018/10/17/5bc726fe87ae2.png)\n",
    "\n",
    "\n",
    "\n",
    "​                                                      \n",
    "​      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;前面花了很大篇幅在说机器为何能学习，接下来要说的是机器是怎么学习的，进入算法$$\\mathcal{A}$$的部分。[上一篇](http://beader.me/2014/03/02/noise-and-error/)稍微提到了几个error的衡量方式，接下来的几篇笔记要讲的就是各种error measurement的区别以及针对它们如何设计最优化的算法。通过设计出来的算法，使得机器能够从$$\\mathcal{H}$$(Hypothesis Set)当中挑选可以使得cost function最小的$$h$$作为$$g$$输出。\n",
    "\n",
    "&emsp;&emsp;本篇以众所周知的线性回归为例，从方程的形式、误差的衡量方式、如何最小化$$E_{in}$$的角度出发，并简单分析了Hat Matrix的性质与几何意义，希望对线性回归这一简单的模型有个更加深刻的理解。\n",
    "\n",
    "\n",
    "# 方程的形式：\n",
    "\n",
    "$$\n",
    "h(x)=\\sum_{i=\\color{red}{0}}^d w_ix_i= w^Tx \\\\\\\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;长得很像perceptron(都是直线嘛)，perceptron是$$h(x)=sign(w^Tx)$$。\n",
    "\n",
    "# 误差的衡量 — 平方误差(squared error)：\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "err(\\hat{y}_n,y_n) = (\\hat{y}_n-y_n)^2\\\\\\\n",
    "(\\hat{y}_n\\text{为预测值，}y_n\\text{为真实值})\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "# Cost function：\n",
    "\n",
    "$$\n",
    "E_{in}(w)=\\frac{1}{N}\\sum_{n=1}^N(\\hat{y}_n - y_n)=\\frac{1}{N}\\sum_{n=1}^N(w^Tx_n-y_n)^2\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$$h(x)$$是一个以$$x$$为变量的方程，而$$E_{in}(w)$$变成了一个以$$w$$为变量的方程。这样一来，我们就把“在$$\\mathcal{H}$$中寻找能使平均误差最小的方程”这个问题，转换为“求解一个函数的最小值”的问题。使得$$E_{in}(w)$$最小的$$w$$，就是我们要寻找的那个最优方程的参数。\n",
    "\n",
    "# 如何最小化$$E_{in}(w)$$：\n",
    "\n",
    "&emsp;&emsp;用矩阵形式表示：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{in}(\\color{blue}{w}) &= \\frac{1}{N}\\sum_{n=1}^{N}(\\color{blue}{w^T}\\color{red}{x_n}-\\color{purple}{y_n})^2=\\frac{1}{N}\\sum_{n=1}^{N}(\\color{red}{x_n^T}\\color{blue}{w}-\\color{purple}{y_n})^2 \\\\\n",
    "&=\\frac{1}{N}\\begin{Vmatrix}\n",
    "\\color{red}{x_1^T}\\color{blue}{w}-\\color{purple}{y_1}\\\\\n",
    "\\color{red}{x_2^T}\\color{blue}{w}-\\color{purple}{y_2}\\\\\n",
    "...\\\\\n",
    "\\color{red}{x_N^T}\\color{blue}{w}-\\color{purple}{y_N}\n",
    "\\end{Vmatrix}^2 \\\\\n",
    "\n",
    "&=\\frac{1}{N}\\begin{Vmatrix}\n",
    "\\color{red}{\\begin{bmatrix}\n",
    "--x_1^T--\\\\\n",
    "--x_2^T--\\\\\n",
    "...\\\\\n",
    "--x_N^T--\n",
    "\\end{bmatrix}}\n",
    "\\color{blue}{w} -\n",
    "\\color{purple}{\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "...\\\\\n",
    "y_3\n",
    "\\end{bmatrix}}\n",
    "\\end{Vmatrix}^2 \\\\\n",
    "\n",
    "&=\\frac{1}{N}||\n",
    "\\underbrace{\\color{red}{X}}_{N\\times d+1}\\;\\;\\;\n",
    "\\underbrace{\\color{blue}{w}}_{d+1\\times 1} \\; - \\;\n",
    "\\underbrace{\\color{purple}{y}}_{N\\times 1}\n",
    "||^2\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$$\\color{red}{X}$$与$$\\color{purple}{y}$$来源于$$\\mathcal{D}$$，是固定不变的，因此它是一个以$$\\color{blue}{w}$$为变量的函数。我们需要解使得$$E_{in}$$最小的$$\\color{blue}{w}$$，即$$\\underset{\\color{blue}{w}}{min}\\,E_{in}(\\color{blue}{w})=\\frac{1}{N}\\begin{Vmatrix}\\color{red}{X}\\color{blue}{w}-\\color{purple}{y}\\end{Vmatrix}^2$$。这个$$E_{in}(\\color{blue}{w})$$是一个连续(continuous)、处处可微(differentiable)的凸函数(convex)：\n",
    "\n",
    "  ![](images/wlin.png) &emsp;&emsp;\n",
    "\n",
    "&emsp;&emsp;对于这一类函数，只需要解其一阶导数为0时的解即可。\n",
    "\n",
    "$$\n",
    "E_{in}(\\color{blue}{w}) \\equiv\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial E_{in}}{\\partial \\color{blue}{w}_0}(\\color{blue}{w})\\\\\\\n",
    "\\frac{\\partial E_{in}}{\\partial \\color{blue}{w}_1}(\\color{blue}{w})\\\\\\\n",
    "...\\\\\\\n",
    "\\frac{\\partial E_{in}}{\\partial \\color{blue}{w}_d}(\\color{blue}{w})\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\color{orange}{0}\\\\\\\n",
    "\\color{orange}{0}\\\\\\\n",
    "...\\\\\\\n",
    "\\color{orange}{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;关于多元函数的求导，就是线性代数的范畴了：\n",
    "\n",
    "$$\n",
    "\\boxed\n",
    "{\n",
    "\\begin{matrix}\n",
    "\\text{一元的情况}\\\\\\\n",
    "\\\\\\\n",
    "E_{in}(\\color{blue}{w})=\\frac{1}{N}(\\color{red}{a}\\color{blue}{w^2}-2\\color{brown}{b}\\color{blue}{w}+\\color{purple}{c})\\\\\\\n",
    "\\nabla E_{in}(\\color{blue}{w})=\\frac{1}{N}(2\\color{red}{a}\\color{blue}{w}-2\\color{brown}{b})\n",
    "\\end{matrix}\n",
    "}\n",
    "\\xrightarrow{\\text{推广至}}\n",
    "\\boxed{\n",
    "\\begin{matrix}\n",
    "\\text{多元的情况}\\\\\\\n",
    "\\\\\\\n",
    "E_{in}(\\color{blue}{w})=\\frac{1}{N}(\\color{blue}{w^T}\\color{red}{A}\\color{blue}{w}-2\\color{blue}{w^T}\\color{brown}{b}+\\color{purple}{c})\\\\\\\n",
    "\\nabla E_{in}(\\color{blue}{w})=\\frac{1}{N}(2\\color{red}{A}\\color{blue}{w}-2\\color{brown}{b})\n",
    "\\end{matrix}\n",
    "}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;所以有：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla E_{in}(\\color{blue}{w}) &=\\nabla \\frac{1}{N}(\\color{blue}{w^T}\\color{red}{X^TX}\\color{blue}{w}-2\\color{blue}{w^T}\\color{brown}{X^Ty}+\\color{purple}{y^Ty}) \\\\\\\n",
    "&=\\frac{2}{N}(\\color{red}{X^TX}\\color{blue}{w}-\\color{brown}{X^Ty})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;令$$\\nabla E_{in}(\\color{blue}{w})=0$$，可得最佳解：\n",
    "\n",
    "$$\n",
    "\\color{blue}{w_{LIN}}=\\underbrace{(\\color{red}{X^TX})^{-1}\\color{red}{X^T}}_{pseudo-inverse\\;\\color{red}{X^{\\dagger}}}\\;\\;\\;\\color{purple}{y} = \\color{red}{X^{\\dagger}} \\color{purple}{y}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;当$$\\color{red}{X^TX}$$可逆的时候用它作为pseudo-inverse矩阵$$\\color{red}{X^{\\dagger}}$$，当$$\\color{red}{X^TX}$$不可逆的时候，再用其他方式定义$$\\color{red}{X^{\\dagger}}$$，这里就不详述了。\n",
    "\n",
    "&emsp;&emsp;用以$$\\color{blue}{w_{LIN}}$$为参数的线性方程对原始数据做预测，可以得到拟合值$$\\hat{y}=\\color{red}{X}\\color{blue}{w_{LIN}}=\\color{red}{XX^{\\dagger}}\\color{purple}{y}$$。这里又称$$\\color{orange}{H}=\\color{red}{XX^{\\dagger}}$$为Hat Matrix，帽子矩阵，$$\\color{orange}{H}$$为$$\\color{purple}{y}$$带上了帽子，成为$$\\hat{y}$$，很形象吧。\n",
    "\n",
    "# Hat Matrix 的几何意义\n",
    "\n",
    "![](images/geoview_hatmatrix.png)\n",
    "\n",
    "&emsp;&emsp;这张图展示的是在N维实数空间$$\\mathbb{R}^N$$中，注意这里是N=数据笔数，$$\\color{purple}{y}$$中包含所有真实值，$$\\hat{y}$$中包含所有预测值，与之前讲的输入空间是d+1维是不一样的噢。$$\\color{red}{X}$$中包含d+1个column：\n",
    "\n",
    " - $$\\hat{y}=\\color{red}{X}\\color{blue}{w_{LIN}}$$是$$\\color{red}{X}$$的一个线性组合，$$\\color{red}{X}$$中每个column对应$$\\mathbb{R}^N$$下的一个向量，共有d+1个这样的向量，因此$$\\hat{y}$$在这d+1个向量所构成的$$\\color{red}{span}$$(平面)上。\n",
    " - 事实上我们要做的就是在这个平面上找到一个向量$$\\hat{y}$$使得他与真实值之间的距离$$|\\color{green}{y-\\hat{y}}|$$最短。不难发现当$$\\hat{y}$$是$$\\color{purple}{y}$$在这个平面上的投影时，即$$\\color{green}{y-\\hat{y}}\\perp \\color{red}{span}$$时，$$|\\color{green}{y-\\hat{y}}|$$最短。\n",
    " - 所以之前说过的Hat Matrix $$\\color{orange}{H}$$，为$$\\color{purple}{y}$$戴上帽子，所做的就是投影这个动作，寻找$$\\color{red}{span}$$上$$\\color{purple}{y}$$的投影。\n",
    " - $$\\color{orange}{H}\\color{purple}{y}=\\hat{y}$$，$$(I-\\color{orange}{H})\\color{purple}{y}=\\color{green}{y-\\hat{y}}$$。($$I$$为单位矩阵)\n",
    "\n",
    "&emsp;&emsp;下面来探究一下$$\\color{orange}{H}$$的性质，这个很重要噢。\n",
    "\n",
    "$$\n",
    "\\text{Hat Matrix }\\color{orange}{H} = \\color{red}{X(X^TX)}^{-1}\\color{red}{X^T}\n",
    "$$\n",
    "\n",
    " - 对称性(symetric)，即$$\\color{orange}{H}=\\color{orange}{H^T}$$：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{orange}{H^T} &= (\\color{red}{X(X^TX)}^{-1}\\color{red}{X^T})^T \\\\\\\n",
    "&=\\color{red}{X({(X^TX)}^{-1})^TX^T} \\\\\\\n",
    "&=\\color{red}{\\color{red}{X(X^TX)}^{-1}\\color{red}{X^T}}\\\\\\\n",
    "&=\\color{orange}{H}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " - 幂等性(idempotent)，即$$\\color{orange}{H^2}=\\color{orange}{H}$$：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{orange}{H^2} &= (\\color{red}{X(X^TX)}^{-1}\\color{red}{X^T})(\\color{red}{X(X^TX)}^{-1}\\color{red}{X^T})\\\\\\\n",
    "&=\\color{red}{X\\;}\\underbrace{\\color{red}{(X^TX)}^{-1}\\color{red}{(X^TX)}}_{I}\\;\\color{red}{(X^TX)}^{-1}\\color{red}{X^T} \\\\\\\n",
    "&=\\color{red}{X}\\color{red}{(X^TX)}^{-1}\\color{red}{X^T}\\\\\\\n",
    "&=\\color{orange}{H}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " - 半正定(positive semi-definite)，即所有特征值为非负数：  \n",
    "(以下$$\\lambda$$为特征值，$$b$$为对应的特征向量)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{orange}{H}b&=\\lambda b\\\\\\\n",
    "\\color{orange}{H^2}b&=\\lambda \\color{orange}{H}b \\\\\\\n",
    "&=\\lambda (\\lambda b)\\\\\\\n",
    "\\text{(因为}\\color{orange}{H^2}&=\\color{orange}{H}\\text{)}\\\\\\\n",
    "\\color{orange}{H^2}b&=\\color{orange}{H}b=\\lambda b\\\\\\\n",
    "\\text{所以}&:\\\\\\\n",
    "\\lambda ^2b&=\\lambda b \\\\\\\n",
    "\\text{即}&:\\\\\\\n",
    "\\lambda (\\lambda -1)b&=0 \\\\\\\n",
    "\\lambda = 0 &\\text{ or } \\lambda=1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;林老师在课堂上讲到：\n",
    "\n",
    "$$\n",
    "trace(I-\\color{orange}{H}) = N-(d+1)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$$trace$$为矩阵的迹。这条性质很重要，但是为什么呢？证明过程有点多，以后有机会再补充，心急的同学可以看这里[General formulas for bias and variance in OLS][4]。一个矩阵的$$trace$$等于该矩阵的所有特征值(Eigenvalues)之和。\n",
    "\n",
    "![](images/geoview_hatmatrix_noise.png)\n",
    "\n",
    "&emsp;&emsp;假设$$\\color{purple}{y}$$由$$\\color{red}{f(X)\\in span}+noise$$构成的。有$$\\color{purple}{y}=\\color{red}{f(X)}+noise$$。之前讲到$$\\color{orange}{H}$$作用于某个向量，会得到该向量在$$\\color{red}{span}$$上的投影，而$$I-\\color{orange}{H}$$作用于某个向量，会得到那条与$$\\color{red}{span}$$垂直的向量，在这里就是图中的$$\\color{green}{y-\\hat{y}}$$，即$$(I-\\color{orange}{H})noise=\\color{green}{y-\\hat{y}}$$。\n",
    "\n",
    "&emsp;&emsp;这个$$\\color{green}{y-\\hat{y}}$$是真实值与预测值的差，其长度就是就是所有点的平方误差之和。于是就有：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{in}(\\color{blue}{w_{LIN}})&=\\frac{1}{N}||\\color{green}{y-\\hat{y}}||^2\\\\\\\n",
    "&=\\frac{1}{N}||(I-\\color{orange}{H})noise||^2 \\\\\\\n",
    "&=\\frac{1}{N}trace(I-\\color{orange}{H})||noise||^2 \\\\\\\n",
    "&=\\frac{1}{N}(N-(d+1))||noise||^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;上面的证明不太好整理进来，依然可以参考[General formulas for bias and variance in OLS](http://www.stat.berkeley.edu/~census/general.pdf)。\n",
    "\n",
    "&emsp;&emsp;因此，就平均而言，有：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{red}{\\overline{E_{in}}}&=\\text{noise level}\\cdot(1-\\frac{d+1}{N})\\\\\\\n",
    "\\color{blue}{\\overline{E_{out}}}&=\\text{noise level}\\cdot(1+\\frac{d+1}{N}) \\;\\;\\;(后面这个不懂证了。)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;花这么大力气是为了什么，又回到之前learning可行性的话题了。\n",
    "\n",
    "  ![](images/linear_regression_learning_curve.png)\n",
    "\n",
    "&emsp;&emsp;$$\\color{red}{\\overline{E_{in}}}$$和$$\\color{blue}{\\overline{E_{out}}}$$都向$$\\sigma ^2$$(noise level)收敛，并且他们之间的差异被$$\\frac{2(d+1)}{N}$$给bound住了。有那么点像VC bound，不过要比VC bound来的更严格一些。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5\n",
    "$$\\cfrac{\\partial E_{(w, b)}}{\\partial w}=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right)$$\n",
    "[推导]：已知$E_{(w, b)}=\\sum\\limits_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$，所以\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial E_{(w, b)}}{\\partial w}&=\\cfrac{\\partial}{\\partial w} \\left[\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&= \\sum_{i=1}^{m}\\cfrac{\\partial}{\\partial w} \\left[\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&= \\sum_{i=1}^{m}\\left[2\\cdot\\left(y_{i}-w x_{i}-b\\right)\\cdot (-x_i)\\right] \\\\\n",
    "&= \\sum_{i=1}^{m}\\left[2\\cdot\\left(w x_{i}^2-y_i x_i +bx_i\\right)\\right] \\\\\n",
    "&= 2\\cdot\\left(w\\sum_{i=1}^{m} x_{i}^2-\\sum_{i=1}^{m}y_i x_i +b\\sum_{i=1}^{m}x_i\\right) \\\\\n",
    "&=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 3.6\n",
    "$$\\cfrac{\\partial E_{(w, b)}}{\\partial b}=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)$$\n",
    "[推导]：已知$E_{(w, b)}=\\sum\\limits_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$，所以\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial E_{(w, b)}}{\\partial b}&=\\cfrac{\\partial}{\\partial b} \\left[\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\cfrac{\\partial}{\\partial b} \\left[\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[2\\cdot\\left(y_{i}-w x_{i}-b\\right)\\cdot (-1)\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[2\\cdot\\left(b-y_{i}+w x_{i}\\right)\\right] \\\\\n",
    "&=2\\cdot\\left[\\sum_{i=1}^{m}b-\\sum_{i=1}^{m}y_{i}+\\sum_{i=1}^{m}w x_{i}\\right] \\\\\n",
    "&=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 3.7\n",
    "$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "[推导]：令公式(3.5)等于0\n",
    "$$ 0 = w\\sum_{i=1}^{m}x_i^2-\\sum_{i=1}^{m}(y_i-b)x_i $$\n",
    "$$ w\\sum_{i=1}^{m}x_i^2 = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}bx_i $$\n",
    "由于令公式(3.6)等于0可得$b=\\cfrac{1}{m}\\sum_{i=1}^{m}(y_i-wx_i)$，又因为$\\cfrac{1}{m}\\sum_{i=1}^{m}y_i=\\bar{y}$，$\\cfrac{1}{m}\\sum_{i=1}^{m}x_i=\\bar{x}$，则$b=\\bar{y}-w\\bar{x}$，代入上式可得\n",
    "$$\\begin{aligned}\t \n",
    "w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}(\\bar{y}-w\\bar{x})x_i \\\\\n",
    "w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i+w\\bar{x}\\sum_{i=1}^{m}x_i \\\\\n",
    "w(\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i) & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i \\\\\n",
    "w & = \\cfrac{\\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i}\n",
    "\\end{aligned}$$\n",
    "由于$\\bar{y}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}y_i\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i$，$\\bar{x}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}x_i\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2$，代入上式即可得公式(3.7)\n",
    "$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "如果要想用Python来实现上式的话，上式中的求和运算只能用循环来实现，但是如果我们能将上式给向量化，也就是转换成矩阵（向量）运算的话，那么我们就可以利用诸如NumPy这种专门加速矩阵运算的类库来进行编写。下面我们就尝试将上式进行向量化，将$ \\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2=\\bar{x}\\sum_{i=1}^{m}x_i $代入分母可得\n",
    "$$\\begin{aligned}\t  \n",
    "w & = \\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\\\\n",
    "& = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x})}\n",
    "\\end{aligned}$$\n",
    "又因为$ \\bar{y}\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i=\\sum_{i=1}^{m}\\bar{y}x_i=\\sum_{i=1}^{m}\\bar{x}y_i=m\\bar{x}\\bar{y}=\\sum_{i=1}^{m}\\bar{x}\\bar{y} $，$\\sum_{i=1}^{m}x_i\\bar{x}=\\bar{x}\\sum_{i=1}^{m}x_i=\\bar{x}\\cdot m \\cdot\\frac{1}{m}\\cdot\\sum_{i=1}^{m}x_i=m\\bar{x}^2=\\sum_{i=1}^{m}\\bar{x}^2$，则上式可化为\n",
    "$$\\begin{aligned}\n",
    "w & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x}-x_i\\bar{y}+\\bar{x}\\bar{y})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x}-x_i\\bar{x}+\\bar{x}^2)} \\\\\n",
    "& = \\cfrac{\\sum_{i=1}^{m}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{m}(x_i-\\bar{x})^2} \n",
    "\\end{aligned}$$\n",
    "若令$\\boldsymbol{x}=(x_1,x_2,...,x_m)^T$，$\\boldsymbol{x}_{d}=(x_1-\\bar{x},x_2-\\bar{x},...,x_m-\\bar{x})^T$为去均值后的$\\boldsymbol{x}$，$\\boldsymbol{y}=(y_1,y_2,...,y_m)^T$，$\\boldsymbol{y}_{d}=(y_1-\\bar{y},y_2-\\bar{y},...,y_m-\\bar{y})^T$为去均值后的$\\boldsymbol{y}$，其中$\\boldsymbol{x}$、$\\boldsymbol{x}_{d}$、$\\boldsymbol{y}$、$\\boldsymbol{y}_{d}$均为m行1列的列向量，代入上式可得\n",
    "$$w=\\cfrac{\\boldsymbol{x}_{d}^T\\boldsymbol{y}_{d}}{\\boldsymbol{x}_d^T\\boldsymbol{x}_{d}}$$\n",
    "\n",
    "## 3.9\n",
    "$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n",
    "[推导]：公式(3.4)是最小二乘法运用在一元线性回归上的情形，那么对于多元线性回归来说，我们可以类似得到\n",
    "$$\\begin{aligned}\n",
    "\t\\left(\\boldsymbol{w}^{*}, b^{*}\\right)&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}\\right)^{2} \\\\\n",
    "\t&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}\\\\\n",
    "\t&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\left(\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}_{i}+b\\right)\\right)^{2}\n",
    "\\end{aligned}$$\n",
    "为便于讨论，我们令$\\hat{\\boldsymbol{w}}=(\\boldsymbol{w};b)=(w_1;...;w_d;b)\\in\\mathbb{R}^{(d+1)\\times 1},\\hat{\\boldsymbol{x}}_i=(x_1;...;x_d;1)\\in\\mathbb{R}^{(d+1)\\times 1}$，那么上式可以简化为\n",
    "$$\\begin{aligned}\n",
    "\t\\hat{\\boldsymbol{w}}^{*}&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\hat{\\boldsymbol{w}}^\\mathrm{T}\\hat{\\boldsymbol{x}}_{i}\\right)^{2} \\\\\n",
    "\t&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\hat{\\boldsymbol{x}}_{i}^\\mathrm{T}\\hat{\\boldsymbol{w}}\\right)^{2} \\\\\n",
    "\\end{aligned}$$\n",
    "根据向量内积的定义可知，上式可以写成如下向量内积的形式\n",
    "$$\\begin{aligned}\n",
    "\t\\hat{\\boldsymbol{w}}^{*}&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\begin{bmatrix}\n",
    "\ty_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} & \\cdots & y_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\ty_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\vdots \\\\\n",
    "\ty_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n",
    "\t\\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "其中\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "\ty_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\vdots \\\\\n",
    "\ty_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n",
    "\\end{bmatrix}&=\\begin{bmatrix}\n",
    "\ty_{1} \\\\\n",
    "\t\\vdots \\\\\n",
    "\ty_{m}\n",
    "\\end{bmatrix}-\\begin{bmatrix}\n",
    "\t\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\vdots \\\\\n",
    "\t\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\boldsymbol{y}-\\begin{bmatrix}\n",
    "\t\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T} \\\\\n",
    "\t\\vdots \\\\\n",
    "\t\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\n",
    "\\end{bmatrix}\\cdot\\hat{\\boldsymbol{w}}\\\\\n",
    "&=\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol{w}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "所以\n",
    "$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n",
    "\n",
    "## 3.10\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}=2\\mathbf{X}^{\\mathrm{T}}(\\mathbf{X}\\hat{\\boldsymbol w}-\\boldsymbol{y})$$\n",
    "[推导]：将$E_{\\hat{\\boldsymbol w}}=(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol w})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol w})$展开可得\n",
    "$$E_{\\hat{\\boldsymbol w}}= \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}-\\boldsymbol{y}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}-\\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}+\\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}$$\n",
    "对$\\hat{\\boldsymbol w}$求导可得\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}= \\cfrac{\\partial \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}}{\\partial \\hat{\\boldsymbol w}}-\\cfrac{\\partial \\boldsymbol{y}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}}{\\partial \\hat{\\boldsymbol w}}-\\cfrac{\\partial \\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}}{\\partial \\hat{\\boldsymbol w}}+\\cfrac{\\partial \\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}}{\\partial \\hat{\\boldsymbol w}}$$\n",
    "由矩阵微分公式$\\cfrac{\\partial\\boldsymbol{a}^{\\mathrm{T}}\\boldsymbol{x}}{\\partial\\boldsymbol{x}}=\\cfrac{\\partial\\boldsymbol{x}^{\\mathrm{T}}\\boldsymbol{a}}{\\partial\\boldsymbol{x}}=\\boldsymbol{a},\\cfrac{\\partial\\boldsymbol{x}^{\\mathrm{T}}\\mathbf{A}\\boldsymbol{x}}{\\partial\\boldsymbol{x}}=(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}})\\boldsymbol{x}$可得\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}= 0-\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}-\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}+(\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}+\\mathbf{X}^{\\mathrm{T}}\\mathbf{X})\\hat{\\boldsymbol w}$$\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}=2\\mathbf{X}^{\\mathrm{T}}(\\mathbf{X}\\hat{\\boldsymbol w}-\\boldsymbol{y})$$\n",
    "\n",
    "## 3.27\n",
    "$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}(-y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i+\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})) $$\n",
    "[推导]：将公式(3.26)代入公式(3.25)可得\n",
    "$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\ln\\left(y_ip_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})+(1-y_i)p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right) $$\n",
    "其中$ p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\cfrac{e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}},p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\cfrac{1}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}} $，代入上式可得\n",
    "$$\\begin{aligned} \n",
    "\\ell(\\boldsymbol{\\beta})&=\\sum_{i=1}^{m}\\ln\\left(\\cfrac{y_ie^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}+1-y_i}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}\\right) \\\\\n",
    "&=\\sum_{i=1}^{m}\\left(\\ln(y_ie^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}+1-y_i)-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) \n",
    "\\end{aligned}$$\n",
    "由于$ y_i $=0或1，则\n",
    "$$ \\ell(\\boldsymbol{\\beta}) =\n",
    "\\begin{cases} \n",
    "\\sum_{i=1}^{m}(-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})),  & y_i=0 \\\\\n",
    "\\sum_{i=1}^{m}(\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})), & y_i=1\n",
    "\\end{cases} $$\n",
    "两式综合可得\n",
    "$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) $$\n",
    "由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号即可得公式(3.27)。值得一提的是，若将公式(3.26)这个似然项改写为$p(y_i|\\boldsymbol x_i;\\boldsymbol w,b)=[p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{y_i}[p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{1-y_i}$，再将其代入公式(3.25)可得\n",
    "$$\\begin{aligned}\n",
    " \\ell(\\boldsymbol{\\beta})&=\\sum_{i=1}^{m}\\ln\\left([p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{y_i}[p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{1-y_i}\\right) \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)+(1-y_i)\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right] \\\\\n",
    "&=\\sum_{i=1}^{m} \\left \\{ y_i\\left[\\ln\\left(p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)-\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right]+\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right\\} \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(\\cfrac{p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})}{p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})}\\right)+\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}\\right)+\\ln\\left(\\cfrac{1}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}\\right)\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\left(y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) \n",
    "\\end{aligned}$$\n",
    "显然，此种方式更易推导出公式(3.27)\n",
    "\n",
    "## 3.30\n",
    "$$\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}=-\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(y_i-p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta}))$$\n",
    "[解析]：此式可以进行向量化，令$p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\hat{y}_i$，代入上式得\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} &= -\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(y_i-\\hat{y}_i) \\\\\n",
    "& =\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(\\hat{y}_i-y_i) \\\\\n",
    "& ={\\mathbf{X}^{\\mathrm{T}}}(\\hat{\\boldsymbol y}-\\boldsymbol{y}) \\\\\n",
    "& ={\\mathbf{X}^{\\mathrm{T}}}(p_1(\\mathbf{X};\\boldsymbol{\\beta})-\\boldsymbol{y}) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 3.32\n",
    "$$J=\\cfrac{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w}$$\n",
    "[推导]：\n",
    "$$\\begin{aligned}\n",
    "\tJ &= \\cfrac{\\|\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{0}-\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{1}\\|_2^2}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n",
    "\t&= \\cfrac{\\|(\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{0}-\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\|_2^2}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n",
    "\t&= \\cfrac{\\|(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w\\|_2^2}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n",
    "\t&= \\cfrac{\\left[(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w\\right]^{\\mathrm{T}}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n",
    "\t&= \\cfrac{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w}\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 3.37\n",
    "$$\\mathbf{S}_b\\boldsymbol w=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n",
    "[推导]：由公式(3.36)可得拉格朗日函数为\n",
    "$$L(\\boldsymbol w,\\lambda)=-\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_b\\boldsymbol w+\\lambda(\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_w\\boldsymbol w-1)$$\n",
    "对$\\boldsymbol w$求偏导可得\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial L(\\boldsymbol w,\\lambda)}{\\partial \\boldsymbol w} &= -\\cfrac{\\partial(\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_b\\boldsymbol w)}{\\partial \\boldsymbol w}+\\lambda \\cfrac{\\partial(\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_w\\boldsymbol w-1)}{\\partial \\boldsymbol w} \\\\\n",
    "&= -(\\mathbf{S}_b+\\mathbf{S}_b^{\\mathrm{T}})\\boldsymbol w+\\lambda(\\mathbf{S}_w+\\mathbf{S}_w^{\\mathrm{T}})\\boldsymbol w\n",
    "\\end{aligned}$$\n",
    "由于$\\mathbf{S}_b=\\mathbf{S}_b^{\\mathrm{T}},\\mathbf{S}_w=\\mathbf{S}_w^{\\mathrm{T}}$，所以\n",
    "$$\\cfrac{\\partial L(\\boldsymbol w,\\lambda)}{\\partial \\boldsymbol w} = -2\\mathbf{S}_b\\boldsymbol w+2\\lambda\\mathbf{S}_w\\boldsymbol w$$\n",
    "令上式等于0即可得\n",
    "$$-2\\mathbf{S}_b\\boldsymbol w+2\\lambda\\mathbf{S}_w\\boldsymbol w=0$$\n",
    "$$\\mathbf{S}_b\\boldsymbol w=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n",
    "$$(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol{w}=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n",
    "若令$(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol{w}=\\gamma$，则\n",
    "$$\\gamma(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n",
    "$$\\boldsymbol{w}=\\frac{\\gamma}{\\lambda}\\mathbf{S}_{w}^{-1}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$$\n",
    "由于最终要求解的$\\boldsymbol{w}$不关心其大小，只关心其方向，所以$\\frac{\\gamma}{\\lambda}$这个常数项可以任意取值，西瓜书中所说的“不妨令$\\mathbf{S}_b\\boldsymbol w=\\lambda(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$”就等价于令$\\frac{\\gamma}{\\lambda}=1$，此时求解出的$\\boldsymbol{w}$即为公式(3.39)\n",
    "\n",
    "## 3.38\n",
    "$$\\mathbf{S}_b\\boldsymbol{w}=\\lambda(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$$\n",
    "[推导]：参见公式(3.37)\n",
    "\n",
    "## 3.39\n",
    "$$\\boldsymbol{w}=\\mathbf{S}_{w}^{-1}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$$\n",
    "[推导]：参见公式(3.37)\n",
    "\n",
    "## 3.43\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{S}_b &= \\mathbf{S}_t - \\mathbf{S}_w \\\\\n",
    "&= \\sum_{i=1}^N m_i(\\boldsymbol\\mu_i-\\boldsymbol\\mu)(\\boldsymbol\\mu_i-\\boldsymbol\\mu)^{\\mathrm{T}}\n",
    "\\end{aligned}$$\n",
    "[推导]：由公式(3.40)、公式(3.41)、公式(3.42)可得：\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{S}_b &= \\mathbf{S}_t - \\mathbf{S}_w \\\\\n",
    "&= \\sum_{i=1}^m(\\boldsymbol x_i-\\boldsymbol\\mu)(\\boldsymbol x_i-\\boldsymbol\\mu)^{\\mathrm{T}}-\\sum_{i=1}^N\\sum_{\\boldsymbol x\\in X_i}(\\boldsymbol x-\\boldsymbol\\mu_i)(\\boldsymbol x-\\boldsymbol\\mu_i)^{\\mathrm{T}} \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left((\\boldsymbol x-\\boldsymbol\\mu)(\\boldsymbol x-\\boldsymbol\\mu)^{\\mathrm{T}}-(\\boldsymbol x-\\boldsymbol\\mu_i)(\\boldsymbol x-\\boldsymbol\\mu_i)^{\\mathrm{T}}\\right)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left((\\boldsymbol x-\\boldsymbol\\mu)(\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu^{\\mathrm{T}})-(\\boldsymbol x-\\boldsymbol\\mu_i)(\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu_i^{\\mathrm{T}})\\right)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left(\\boldsymbol x\\boldsymbol x^{\\mathrm{T}} - \\boldsymbol x\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol\\mu\\boldsymbol x^{\\mathrm{T}}+\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol x\\boldsymbol x^{\\mathrm{T}}+\\boldsymbol x\\boldsymbol\\mu_i^{\\mathrm{T}}+\\boldsymbol\\mu_i\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left(- \\boldsymbol x\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol\\mu\\boldsymbol x^{\\mathrm{T}}+\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+\\boldsymbol x\\boldsymbol\\mu_i^{\\mathrm{T}}+\\boldsymbol\\mu_i\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(-\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol x\\boldsymbol\\mu^{\\mathrm{T}}-\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu\\boldsymbol x^{\\mathrm{T}}+\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol x\\boldsymbol\\mu_i^{\\mathrm{T}}+\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu_i\\boldsymbol x^{\\mathrm{T}}-\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(-m_i\\boldsymbol\\mu_i\\boldsymbol\\mu^{\\mathrm{T}}-m_i\\boldsymbol\\mu\\boldsymbol\\mu_i^{\\mathrm{T}}+m_i\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}+m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}-m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(-m_i\\boldsymbol\\mu_i\\boldsymbol\\mu^{\\mathrm{T}}-m_i\\boldsymbol\\mu\\boldsymbol\\mu_i^{\\mathrm{T}}+m_i\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n",
    "&= \\sum_{i=1}^Nm_i\\left(-\\boldsymbol\\mu_i\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol\\mu\\boldsymbol\\mu_i^{\\mathrm{T}}+\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n",
    "&= \\sum_{i=1}^N m_i(\\boldsymbol\\mu_i-\\boldsymbol\\mu)(\\boldsymbol\\mu_i-\\boldsymbol\\mu)^{\\mathrm{T}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 3.44\n",
    "$$\\max\\limits_{\\mathbf{W}}\\cfrac{\n",
    "\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})}{\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})}$$\n",
    "[解析]：此式是公式(3.35)的推广形式，证明如下：\n",
    "设$\\mathbf{W}=(\\boldsymbol w_1,\\boldsymbol w_2,...,\\boldsymbol w_i,...,\\boldsymbol w_{N-1})\\in\\mathbb{R}^{d\\times(N-1)}$，其中$\\boldsymbol w_i\\in\\mathbb{R}^{d\\times 1}$为$d$行1列的列向量，则\n",
    "$$\\left\\{\n",
    "\\begin{aligned}\n",
    "\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})&=\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_b \\boldsymbol w_i \\\\\n",
    "\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})&=\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_w \\boldsymbol w_i\n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "所以公式(3.44)可变形为\n",
    "$$\\max\\limits_{\\mathbf{W}}\\cfrac{\n",
    "\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_b \\boldsymbol w_i}{\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_w \\boldsymbol w_i}$$\n",
    "对比公式(3.35)易知上式即公式(3.35)的推广形式\n",
    "\n",
    "## 3.45\n",
    "$$\\mathbf{S}_b\\mathbf{W}=\\lambda\\mathbf{S}_w\\mathbf{W}$$\n",
    "[推导]：同公式(3.35)一样，我们在此处也固定公式(3.44)的分母为1，那么公式(3.44)此时等价于如下优化问题\n",
    "$$\\begin{array}{cl}\\underset{\\boldsymbol{w}}{\\min} & -\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W}) \\\\ \n",
    "\\text { s.t. } & \\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})=1\\end{array}$$\n",
    "根据拉格朗日乘子法可知，上述优化问题的拉格朗日函数为\n",
    "$$L(\\mathbf{W},\\lambda)=-\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})+\\lambda(\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})-1)$$\n",
    "根据矩阵微分公式$\\cfrac{\\partial}{\\partial \\mathbf{X}} \\text { tr }(\\mathbf{X}^{\\mathrm{T}}  \\mathbf{B} \\mathbf{X})=(\\mathbf{B}+\\mathbf{B}^{\\mathrm{T}})\\mathbf{X}$对上式关于$\\mathbf{W}$求偏导可得\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial L(\\mathbf{W},\\lambda)}{\\partial \\mathbf{W}} &= -\\cfrac{\\partial\\left(\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})\\right)}{\\partial \\mathbf{W}}+\\lambda \\cfrac{\\partial\\left(\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})-1\\right)}{\\partial \\mathbf{W}} \\\\\n",
    "&= -(\\mathbf{S}_b+\\mathbf{S}_b^{\\mathrm{T}})\\mathbf{W}+\\lambda(\\mathbf{S}_w+\\mathbf{S}_w^{\\mathrm{T}})\\mathbf{W}\n",
    "\\end{aligned}$$\n",
    "由于$\\mathbf{S}_b=\\mathbf{S}_b^{\\mathrm{T}},\\mathbf{S}_w=\\mathbf{S}_w^{\\mathrm{T}}$，所以\n",
    "$$\\cfrac{\\partial L(\\mathbf{W},\\lambda)}{\\partial \\mathbf{W}} = -2\\mathbf{S}_b\\mathbf{W}+2\\lambda\\mathbf{S}_w\\mathbf{W}$$\n",
    "令上式等于$\\mathbf{0}$即可得\n",
    "$$-2\\mathbf{S}_b\\mathbf{W}+2\\lambda\\mathbf{S}_w\\mathbf{W}=\\mathbf{0}$$\n",
    "$$\\mathbf{S}_b\\mathbf{W}=\\lambda\\mathbf{S}_w\\mathbf{W}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
