{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41e5bca",
   "metadata": {},
   "source": [
    "# linear model\n",
    "#### -note:机器学习的过程就是，通过设计出来的算法，使得机器能够从(Hypothesis Set)当中挑选可以使得cost function最小的ℎ作为𝑔输出。\n",
    "## 3.1 线性回归\n",
    "\n",
    "线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值\\\n",
    "-输入：一个样本数据 $x_i=(x_1,x_2,\\dots,x_d)$\\\n",
    "-Hypothesis Set：$f(x)=w^Tx+b = \\hat{w}^T\\hat{x}$\n",
    "##### note:$\\hat{w}^T\\hat{x}$ 中$\\hat{x}=(x_1,x_2,\\dots,x_d,1),\\hat{w}=(w_1,w_2,\\dots,w_d,b)$\n",
    "输出：$f(x_i)$\\\n",
    "目前主要目标就是找到参数$w^T$使得$f(x)$与真实函数尽可能接近\\\n",
    "### 3.1.1 Cost function：\n",
    "##### Cost function：\n",
    "\n",
    "$$\n",
    "E_{in}(w)=\\frac{1}{N}\\sum_{n=1}^N(\\hat{y}_n - y_n)=\\frac{1}{N}\\sum_{i=1}^N( \\hat{w}^T \\hat{x_i}-y_i)^2\n",
    "$$\n",
    "$h(x)$是一个以$x$为变量的方程，而$E_{in}(w)$变成了一个以$w$为变量的方程。这样一来，我们就把“在$\\mathcal{H}$中寻找能使平均误差最小的方程”这个问题，转换为“求解一个函数的最小值”的问题。使得$E_{in}(w)$最小的$w$，就是我们要寻找的那个最优方程的参数。\n",
    "  \n",
    "##### 求解\n",
    "\n",
    "###### 先说明cost function 是凸函数\n",
    "###### 然后其局部极值即是全局最值，令其梯度为零即得到最优参数\n",
    "###### 一元情况\n",
    "——cost function 是凸函数\\\n",
    "——求梯度\n",
    "$$\\cfrac{\\partial E_{(w, b)}}{\\partial w}=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right)$$\n",
    "[推导]：已知$E_{(w, b)}=\\sum\\limits_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$，所以\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial E_{(w, b)}}{\\partial w}&=\\cfrac{\\partial}{\\partial w} \\left[\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&= \\sum_{i=1}^{m}\\cfrac{\\partial}{\\partial w} \\left[\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&= \\sum_{i=1}^{m}\\left[2\\cdot\\left(y_{i}-w x_{i}-b\\right)\\cdot (-x_i)\\right] \\\\\n",
    "&= \\sum_{i=1}^{m}\\left[2\\cdot\\left(w x_{i}^2-y_i x_i +bx_i\\right)\\right] \\\\\n",
    "&= 2\\cdot\\left(w\\sum_{i=1}^{m} x_{i}^2-\\sum_{i=1}^{m}y_i x_i +b\\sum_{i=1}^{m}x_i\\right) \\\\\n",
    "&=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right)\n",
    "\\end{aligned}$$\n",
    "\\\n",
    "$$\\cfrac{\\partial E_{(w, b)}}{\\partial b}=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)$$\n",
    "[推导]：已知$E_{(w, b)}=\\sum\\limits_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$，所以\n",
    "$$\\begin{aligned}\n",
    "\\cfrac{\\partial E_{(w, b)}}{\\partial b}&=\\cfrac{\\partial}{\\partial b} \\left[\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\cfrac{\\partial}{\\partial b} \\left[\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[2\\cdot\\left(y_{i}-w x_{i}-b\\right)\\cdot (-1)\\right] \\\\\n",
    "&=\\sum_{i=1}^{m}\\left[2\\cdot\\left(b-y_{i}+w x_{i}\\right)\\right] \\\\\n",
    "&=2\\cdot\\left[\\sum_{i=1}^{m}b-\\sum_{i=1}^{m}y_{i}+\\sum_{i=1}^{m}w x_{i}\\right] \\\\\n",
    "&=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)\n",
    "\\end{aligned}$$\n",
    "——令梯度为0\n",
    "$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "由于E\n",
    "[推导]：令公式(3.5)等于0\n",
    "$$ 0 = w\\sum_{i=1}^{m}x_i^2-\\sum_{i=1}^{m}(y_i-b)x_i $$\n",
    "$$ w\\sum_{i=1}^{m}x_i^2 = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}bx_i $$\n",
    "由于令公式(3.6)等于0可得$b=\\cfrac{1}{m}\\sum_{i=1}^{m}(y_i-wx_i)$，又因为$\\cfrac{1}{m}\\sum_{i=1}^{m}y_i=\\bar{y}$，$\\cfrac{1}{m}\\sum_{i=1}^{m}x_i=\\bar{x}$，则$b=\\bar{y}-w\\bar{x}$，代入上式可得\n",
    "$$\\begin{aligned}\t \n",
    "w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}(\\bar{y}-w\\bar{x})x_i \\\\\n",
    "w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i+w\\bar{x}\\sum_{i=1}^{m}x_i \\\\\n",
    "w(\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i) & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i \\\\\n",
    "w & = \\cfrac{\\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i}\n",
    "\\end{aligned}$$\n",
    "由于$\\bar{y}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}y_i\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i$，$\\bar{x}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}x_i\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2$，代入上式即可得公式(3.7)\n",
    "$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "如果要想用Python来实现上式的话，上式中的求和运算只能用循环来实现，但是如果我们能将上式给向量化，也就是转换成矩阵（向量）运算的话，那么我们就可以利用诸如NumPy这种专门加速矩阵运算的类库来进行编写。下面我们就尝试将上式进行向量化，将$ \\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2=\\bar{x}\\sum_{i=1}^{m}x_i $代入分母可得\n",
    "$$\\begin{aligned}\t  \n",
    "w & = \\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\\\\n",
    "& = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x})}\n",
    "\\end{aligned}$$\n",
    "又因为$ \\bar{y}\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i=\\sum_{i=1}^{m}\\bar{y}x_i=\\sum_{i=1}^{m}\\bar{x}y_i=m\\bar{x}\\bar{y}=\\sum_{i=1}^{m}\\bar{x}\\bar{y} $，$\\sum_{i=1}^{m}x_i\\bar{x}=\\bar{x}\\sum_{i=1}^{m}x_i=\\bar{x}\\cdot m \\cdot\\frac{1}{m}\\cdot\\sum_{i=1}^{m}x_i=m\\bar{x}^2=\\sum_{i=1}^{m}\\bar{x}^2$，则上式可化为\n",
    "$$\\begin{aligned}\n",
    "w & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x}-x_i\\bar{y}+\\bar{x}\\bar{y})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x}-x_i\\bar{x}+\\bar{x}^2)} \\\\\n",
    "& = \\cfrac{\\sum_{i=1}^{m}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{m}(x_i-\\bar{x})^2} \n",
    "\\end{aligned}$$\n",
    "若令$\\boldsymbol{x}=(x_1,x_2,...,x_m)^T$，$\\boldsymbol{x}_{d}=(x_1-\\bar{x},x_2-\\bar{x},...,x_m-\\bar{x})^T$为去均值后的$\\boldsymbol{x}$，$\\boldsymbol{y}=(y_1,y_2,...,y_m)^T$，$\\boldsymbol{y}_{d}=(y_1-\\bar{y},y_2-\\bar{y},...,y_m-\\bar{y})^T$为去均值后的$\\boldsymbol{y}$，其中$\\boldsymbol{x}$、$\\boldsymbol{x}_{d}$、$\\boldsymbol{y}$、$\\boldsymbol{y}_{d}$均为m行1列的列向量，代入上式可得\n",
    "$$w=\\cfrac{\\boldsymbol{x}_{d}^T\\boldsymbol{y}_{d}}{\\boldsymbol{x}_d^T\\boldsymbol{x}_{d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98d097",
   "metadata": {},
   "source": [
    "###### 多元情况\n",
    "————cost function\n",
    "$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n",
    "[推导]：公式(3.4)是最小二乘法运用在一元线性回归上的情形，那么对于多元线性回归来说，我们可以类似得到\n",
    "$$\\begin{aligned}\n",
    "\t\\left(\\boldsymbol{w}^{*}, b^{*}\\right)&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}\\right)^{2} \\\\\n",
    "\t&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}\\\\\n",
    "\t&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\left(\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}_{i}+b\\right)\\right)^{2}\n",
    "\\end{aligned}$$\n",
    "为便于讨论，我们令$\\hat{\\boldsymbol{w}}=(\\boldsymbol{w};b)=(w_1;...;w_d;b)\\in\\mathbb{R}^{(d+1)\\times 1},\\hat{\\boldsymbol{x}}_i=(x_1;...;x_d;1)\\in\\mathbb{R}^{(d+1)\\times 1}$，那么上式可以简化为\n",
    "$$\\begin{aligned}\n",
    "\t\\hat{\\boldsymbol{w}}^{*}&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\hat{\\boldsymbol{w}}^\\mathrm{T}\\hat{\\boldsymbol{x}}_{i}\\right)^{2} \\\\\n",
    "\t&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\hat{\\boldsymbol{x}}_{i}^\\mathrm{T}\\hat{\\boldsymbol{w}}\\right)^{2} \\\\\n",
    "\\end{aligned}$$\n",
    "根据向量内积的定义可知，上式可以写成如下向量内积的形式\n",
    "$$\\begin{aligned}\n",
    "\t\\hat{\\boldsymbol{w}}^{*}&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\begin{bmatrix}\n",
    "\ty_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} & \\cdots & y_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\ty_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\vdots \\\\\n",
    "\ty_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n",
    "\t\\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "其中\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "\ty_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\vdots \\\\\n",
    "\ty_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n",
    "\\end{bmatrix}&=\\begin{bmatrix}\n",
    "\ty_{1} \\\\\n",
    "\t\\vdots \\\\\n",
    "\ty_{m}\n",
    "\\end{bmatrix}-\\begin{bmatrix}\n",
    "\t\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n",
    "\t\\vdots \\\\\n",
    "\t\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\boldsymbol{y}-\\begin{bmatrix}\n",
    "\t\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T} \\\\\n",
    "\t\\vdots \\\\\n",
    "\t\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\n",
    "\\end{bmatrix}\\cdot\\hat{\\boldsymbol{w}}\\\\\n",
    "&=\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol{w}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "所以\n",
    "$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n",
    "对\n",
    "cost funtion\n",
    "求梯度得\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}=2\\mathbf{X}^{\\mathrm{T}}(\\mathbf{X}\\hat{\\boldsymbol w}-\\boldsymbol{y})$$\n",
    "\n",
    "[推导]：将$E_{\\hat{\\boldsymbol w}}=(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol w})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol w})$展开可得\n",
    "$$E_{\\hat{\\boldsymbol w}}= \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}-\\boldsymbol{y}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}-\\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}+\\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}$$\n",
    "对$\\hat{\\boldsymbol w}$求导可得\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}= \\cfrac{\\partial \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}}{\\partial \\hat{\\boldsymbol w}}-\\cfrac{\\partial \\boldsymbol{y}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}}{\\partial \\hat{\\boldsymbol w}}-\\cfrac{\\partial \\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}}{\\partial \\hat{\\boldsymbol w}}+\\cfrac{\\partial \\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}}{\\partial \\hat{\\boldsymbol w}}$$\n",
    "由矩阵微分公式$\\cfrac{\\partial\\boldsymbol{a}^{\\mathrm{T}}\\boldsymbol{x}}{\\partial\\boldsymbol{x}}=\\cfrac{\\partial\\boldsymbol{x}^{\\mathrm{T}}\\boldsymbol{a}}{\\partial\\boldsymbol{x}}=\\boldsymbol{a},\\cfrac{\\partial\\boldsymbol{x}^{\\mathrm{T}}\\mathbf{A}\\boldsymbol{x}}{\\partial\\boldsymbol{x}}=(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}})\\boldsymbol{x}$可得\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}= 0-\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}-\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}+(\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}+\\mathbf{X}^{\\mathrm{T}}\\mathbf{X})\\hat{\\boldsymbol w}$$\n",
    "$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}=2\\mathbf{X}^{\\mathrm{T}}(\\mathbf{X}\\hat{\\boldsymbol w}-\\boldsymbol{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d046e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
