{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55f7d0b9",
   "metadata": {},
   "source": [
    "<table>\n",
    "        <tr>\n",
    "            <th>模型\n",
    "            </th>\n",
    "            <th>线性回归\n",
    "            </th>\n",
    "            <th>逻辑回归\n",
    "            </th>\n",
    "            <th>线性判别\n",
    "            </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>输入（样本空间）\n",
    "            </th>\n",
    "            <th>$$x_i=(x_1,x_2,\\dots,x_d)\\ x\\in R^n$$\n",
    "            </th>\n",
    "            <th>$$x_i=(x_1,x_2,\\dots,x_d)$$\n",
    "            </th>\n",
    "            <th>$$x_i=(x_1,x_2,\\dots,x_d)$$\n",
    "            </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>标记空间\n",
    "            </th>\n",
    "            <th>$$y\\in R$$\n",
    "            </th>\n",
    "            <th>$$y\\in \\{-1,1\\}$$\n",
    "            </th>\n",
    "            <th>$$y\\in N$$\n",
    "            </th>\n",
    "        </tr>\n",
    "     <tr>\n",
    "            <th>假设空间\n",
    "            </th>\n",
    "            <th>$$f(x)=w^Tx+b = \\hat{w}^T\\hat{x}$$note:$\\hat{w}^T\\hat{x}$ 中$\\hat{x}=(x_1,x_2,\\dots,x_d,1),\\hat{w}=(w_1,w_2,\\dots,w_d,b)$\n",
    "            </th>\n",
    "            <th>$$ln\\frac{y}{1-y}=w^Tx+b = \\hat{w}^T\\hat{x}$$note:$$\\hat{x}=(x_1,x_2,\\dots,x_d,1),\\hat{w}=(w_1,w_2,\\dots,w_d,b)$$\n",
    "                对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。\n",
    "\n",
    "![9.png](https://i.loli.net/2018/10/17/5bc722b0c7748.png)\n",
    "\n",
    "![10.png](https://i.loli.net/2018/10/17/5bc722b0a655d.png)\n",
    "\n",
    "若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。\n",
    "            </th>\n",
    "            <th>$$y=w^Tx$$ 将样本点投影到这条直线使得同类样本的投影点尽可能接近，异类样本投影点尽可能远离\n",
    "                ![13.png](https://i.loli.net/2018/10/17/5bc723b863ebb.png)\n",
    "                ![14.png](https://i.loli.net/2018/10/17/5bc723b85bfa9.png)\n",
    "                让各类的协方差之和尽可能小，不同类之间中心的距离尽可能大。\n",
    "            </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>策略\n",
    "            </th>\n",
    "            <th>\n",
    "                1.预测值与真实值尽可能接近，即两者之差（残差）的方差尽可能小\n",
    "                $$E_{in}(w)=\\frac{1}{N}\\sum_{n=1}^N(\\hat{y}_n - y_n)=\\frac{1}{N}\\sum_{n=1}^N(w^Tx_n-y_n)^2$$\n",
    "                略去系数（不影响最值条件），向量化（联想二次型的向量化）得到$$E(\\hat{w}) =(y−X\\hat{w})^T(y−X\\hat{w})$$\n",
    "                2.极大似然估计：在预测模型下，使得预测结果出现的概率最大\\\n",
    "                具体上：由残差$ε$符合均值为零的正态分布，利用$ε=y_i-w^Tx_i$得到关于$y_i$的概率分布，为$y∼N(w^Tx_i,σ^2)$\n",
    "                得到：联合概率分布：$L(w,b)=∏_{i=1}^{i=m}p(y_i)$\n",
    "                取对数得到：\n",
    "                <img src=\"imgs/lnL(w,b).png\">\n",
    "                让其最大化，即让负数项最小化，得到\n",
    "                <img src=\"imgs/lnL(w,b)_2.png\">\n",
    "            </th>\n",
    "            <th>\n",
    "                 1.最大似然估计\n",
    "                   $$p(y∣\\hat{x};\\hat{w})=y⋅p(y = 1|\\hat{x};\\hat{w})+(1−y)⋅p(y = 0|\\hat{x};\\hat{w})$$\n",
    "                   $$p(y∣\\hat{x};\\hat{w})=[p(y = 1|\\hat{x};\\hat{w})]^y[(1−y)⋅p(y = 0|\\hat{x};\\hat{w})]^{(1-y)}$$\n",
    "                ![11.png](https://i.loli.net/2018/10/17/5bc723b824f0c.png)\n",
    "                ![12.png](https://i.loli.net/2018/10/17/5bc723b817961.png)\n",
    "                            损失函数为$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}(-y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i+\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})) $$\n",
    "            [推导]：\n",
    "            $$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\ln\\left(y_ip_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})+(1-y_i)p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right) $$\n",
    "            其中$ p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\cfrac{e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}},p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\cfrac{1}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}} $，代入上式可得\n",
    "            $$\\begin{aligned} \n",
    "            \\ell(\\boldsymbol{\\beta})&=\\sum_{i=1}^{m}\\ln\\left(\\cfrac{y_ie^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}+1-y_i}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}\\right) \\\\\n",
    "            &=\\sum_{i=1}^{m}\\left(\\ln(y_ie^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}+1-y_i)-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) \n",
    "            \\end{aligned}$$\n",
    "            由于$ y_i $=0或1，则\n",
    "            $$ \\ell(\\boldsymbol{\\beta}) =\n",
    "            \\begin{cases} \n",
    "            \\sum_{i=1}^{m}(-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})),  & y_i=0 \\\\\n",
    "            \\sum_{i=1}^{m}(\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})), & y_i=1\n",
    "            \\end{cases} $$\n",
    "            两式综合可得\n",
    "            $$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) $$\n",
    "            由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号即可得损失函数。利用条件概率的幂形式$p(y_i|\\boldsymbol x_i;\\boldsymbol w,b)=[p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{y_i}[p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{1-y_i}$，再将其代入公式(3.25)可得\n",
    "            $$\\begin{aligned}\n",
    "             \\ell(\\boldsymbol{\\beta})&=\\sum_{i=1}^{m}\\ln\\left([p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{y_i}[p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{1-y_i}\\right) \\\\\n",
    "            &=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)+(1-y_i)\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right] \\\\\n",
    "            &=\\sum_{i=1}^{m} \\left \\{ y_i\\left[\\ln\\left(p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)-\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right]+\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right\\} \\\\\n",
    "            &=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(\\cfrac{p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})}{p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})}\\right)+\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right] \\\\\n",
    "            &=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}\\right)+\\ln\\left(\\cfrac{1}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}\\right)\\right] \\\\\n",
    "            &=\\sum_{i=1}^{m}\\left(y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) \n",
    "            \\end{aligned}$$\n",
    "                2.信息论角度\n",
    "                <img src=\"imgs/xinxi.png\">\n",
    "            </th>\n",
    "            <th>\n",
    "                让投影后各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。\n",
    "\n",
    "+ 类内散度矩阵（within-class scatter matrix）\n",
    "\n",
    "![15.png](https://i.loli.net/2018/10/17/5bc723b8156e1.png)\n",
    "\n",
    "+ 类间散度矩阵(between-class scaltter matrix)\n",
    "\n",
    "![16.png](https://i.loli.net/2018/10/17/5bc723b7e9db3.png)\n",
    "                由此，定义了$S_b$和$S_w$的广义瑞利商\n",
    "                <img src=\"imgs/costfun-linear_te.png\">\n",
    "            </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>参数求解：1.是否为凸优化问题\n",
    "            </th>\n",
    "            <th>1.一元线性回归是凸优化问题：\n",
    "                求得损失函数的hessian矩阵为半正定矩阵，为凸函数\n",
    "                <img src=\"imgs/V2E(w,b).png\">\n",
    "                <img src=\"imgs/V2E(w,b)2.png\">\n",
    "                1.多元线性回归假定是凸优化问题：\n",
    "                <img src=\"imgs/M_L_t.png\">\n",
    "            </th>\n",
    "            <th>\n",
    "                高阶连续可导凸函数\n",
    "                <img src=\"imgs/convexV2.png\">\n",
    "            </th>\n",
    "            <th>\n",
    "            </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>参数求解：2.解出参数\n",
    "            </th>\n",
    "            <th>1.对损失函数求梯度，令其为零，得到参数解\n",
    "                <img src=\"imgs/M_L_2.png\">\n",
    "                2.利用梯度下降算法求解\n",
    "            </th>\n",
    "            <th>\n",
    "                1.求梯度，令其为零\n",
    "                $$\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}=-\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(y_i-p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta}))$$\n",
    "[注]：此式可以进行向量化，令$p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\hat{y}_i$，代入上式得\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} &= -\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(y_i-\\hat{y}_i) \\\\\n",
    "& =\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(\\hat{y}_i-y_i) \\\\\n",
    "& ={\\mathbf{X}^{\\mathrm{T}}}(\\hat{\\boldsymbol y}-\\boldsymbol{y}) \\\\\n",
    "& ={\\mathbf{X}^{\\mathrm{T}}}(p_1(\\mathbf{X};\\boldsymbol{\\beta})-\\boldsymbol{y}) \\\\\n",
    "\\end{aligned}$$\n",
    "                2.利用梯度下降算法或牛顿法，坐标下降法求解\n",
    "            </th>\n",
    "            <th>1.拉格朗日乘子法\n",
    "                 <img src=\"imgs/lag.png\">\n",
    "            </th>\n",
    "        </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7654d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
